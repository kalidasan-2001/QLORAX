{
  "train_loss": 3.52998468875885,
  "train_runtime": 4.4419,
  "train_samples_per_second": 2.251,
  "total_flos": 655495004160.0,
  "config": {
    "model_name": "gpt2",
    "torch_dtype": "float32",
    "device_map": "cpu",
    "trust_remote_code": false,
    "data_path": "data/training_data.jsonl",
    "validation_split": 0.0,
    "prompt_template": "### Input:\n{input}\n\n### Output:\n{output}",
    "max_length": 128,
    "load_in_4bit": false,
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": false,
    "bnb_4bit_compute_dtype": "float32",
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "lora_target_modules": [
      "c_attn"
    ],
    "lora_bias": "none",
    "num_epochs": 1,
    "per_device_train_batch_size": 1,
    "per_device_eval_batch_size": 1,
    "gradient_accumulation_steps": 1,
    "learning_rate": 5e-05,
    "weight_decay": 0.01,
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "max_grad_norm": 1.0,
    "lr_scheduler_type": "linear",
    "warmup_ratio": 0.1,
    "fp16": false,
    "bf16": false,
    "gradient_checkpointing": false,
    "dataloader_num_workers": 0,
    "output_dir": "test_output",
    "logging_steps": 1,
    "save_steps": 1000,
    "save_total_limit": 1,
    "eval_steps": 1000,
    "early_stopping_patience": 10,
    "use_wandb": false,
    "wandb_project": "qlorax-test",
    "experiment_name": "test-run",
    "seed": 42
  }
}
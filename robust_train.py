#!/usr/bin/env python3
"""
Robust QLoRA training script with better error handling and smaller model option
"""

import os
import json
from pathlib import Path
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_dataset(file_path):
    """Load JSONL dataset"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))
    return data

def create_training_prompt(example):
    """Create training prompt from input/output pair"""
    return f"### Input:\n{example['input']}\n\n### Output:\n{example['output']}\n<|endoftext|>"

def download_model_with_retry(model_name, max_retries=3):
    """Download model with retry logic"""
    for attempt in range(max_retries):
        try:
            print(f"üì• Attempting to load model (attempt {attempt + 1}/{max_retries})...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # Add pad token if missing
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto",
                trust_remote_code=False,
                low_cpu_mem_usage=True
            )
            
            print("‚úÖ Model loaded successfully!")
            return model, tokenizer
            
        except Exception as e:
            print(f"‚ùå Attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                raise e
            print("üîÑ Retrying in 5 seconds...")
            import time
            time.sleep(5)

def main():
    # Configuration - using a very small model for quick testing
    model_options = [
        "microsoft/DialoGPT-small",  # Much smaller, faster download
        "gpt2",  # Even smaller, very fast
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # Original choice
    ]
    
    print("üöÄ Robust QLoRA Training with Transformers + PEFT")
    print("=" * 60)
    
    # Try models in order of preference
    model, tokenizer = None, None
    for model_name in model_options:
        try:
            print(f"üéØ Trying model: {model_name}")
            model, tokenizer = download_model_with_retry(model_name, max_retries=2)
            print(f"‚úÖ Successfully loaded: {model_name}")
            break
        except Exception as e:
            print(f"‚ùå Failed to load {model_name}: {e}")
            continue
    
    if model is None:
        print("‚ùå Failed to load any model. Check your internet connection.")
        return 1
    
    dataset_path = "data/my_custom_dataset.jsonl"
    output_dir = "models/robust-qlora-model"
    
    # Configure LoRA
    print("‚öôÔ∏è Configuring LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=32,  # Smaller rank for faster training
        lora_alpha=16,
        lora_dropout=0.05,
        # Use generic target modules that work with most models
        target_modules=["c_attn", "c_proj", "c_fc"] if "gpt2" in str(model.config._name_or_path) else ["q_proj", "v_proj"],
        bias="none",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # Load and prepare dataset
    print("üìä Loading dataset...")
    raw_data = load_dataset(dataset_path)
    
    # Create training prompts
    train_texts = [create_training_prompt(example) for example in raw_data]
    
    # Tokenize
    def tokenize_function(examples):
        return tokenizer(
            examples["text"], 
            truncation=True, 
            padding=True, 
            max_length=256,  # Shorter for faster training
            return_tensors="pt"
        )
    
    train_dataset = Dataset.from_dict({"text": train_texts})
    train_dataset = train_dataset.map(tokenize_function, batched=True)
    
    # Training arguments - optimized for quick training
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=1,  # Just 1 epoch for quick test
        per_device_train_batch_size=1,
        gradient_accumulation_steps=2,
        warmup_steps=10,
        learning_rate=5e-4,  # Higher LR for faster convergence
        logging_steps=1,
        save_steps=50,
        evaluation_strategy="no",
        save_strategy="steps",
        fp16=False,
        bf16=False,
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        report_to=None,  # Disable W&B
    )
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )
    
    # Start training
    print("üèÉ Starting training...")
    print(f"Dataset size: {len(train_texts)} examples")
    print(f"Model: {model.config._name_or_path}")
    print(f"Output directory: {output_dir}")
    print("=" * 60)
    
    try:
        trainer.train()
        print("‚úÖ Training completed!")
        
        # Save the model
        print("üíæ Saving model...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        print(f"‚úÖ Model saved to: {output_dir}")
        
        # Test the model quickly
        print("\nüß™ Quick test of the trained model:")
        test_input = "What is machine learning?"
        inputs = tokenizer.encode(f"### Input:\n{test_input}\n\n### Output:\n", return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs, 
                max_length=inputs.shape[1] + 50,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"üìù Input: {test_input}")
        print(f"ü§ñ Output: {response}")
        
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    print(f"\nüéØ Training completed with exit code: {exit_code}")
    input("Press Enter to exit...")
    exit(exit_code)